model:
  kind: dl
  name: tcnformer_base
  kwargs:
    # ---- data ----
    # input_size: 12
    # num_classes: 8

    # ---- TCN ----
    dropout: 0.1
    is_batchnorm: true
    num_convs_per_block: 2

    # out channels per block
    num_filters_per_block: [32, 64, 128]

    # each block needs a list length = num_convs_per_block
    kernel_sizes:
      - [3, 3]
      - [3, 3]
      - [3, 3]

    strides:
      - [1, 1]
      - [1, 1]
      - [1, 1]

    dilations:
      - [1, 2]
      - [2, 4]
      - [4, 8]

    # ---- Transformer ----
    transformer_window_size: 50
    transformer_num_heads: 4
    transformer_num_layers: 2
    transformer_dim_feedforward: 256
    transformer_dropout: 0.1
    use_positional_encoding: true

    # ---- Training config (optional, like your style) ----
    train_cfg:
      num_epochs: 50
      optimizer_cfg: {"name": "adam", "lr": 1e-3}
      weight_decay: 1e-4
