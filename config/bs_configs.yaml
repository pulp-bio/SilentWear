# File that defines Fine-Tuning Model Configurations.
# It must be used together with the script subject_inter_sessions_with_ft.py

num_ft_epochs: 100                 # Number of epochs to fine-tune the model for.
ft_lr: 1e-3                      # Learning rate to use for fine-tuning.


# Used only for the baseline models (baseline_models.py)
num_epochs_first_train: 100
lr_first_train: 1e-3

train_all_network_layers: true    # Whether to train all layers of the network or only the last ones.
                                  # If false, only the last layer will be trained.

# Parent directory which should contain the pre-trained model (base).
# If null, re-train the inter-session models from scratch.

model_parent_directory: "/scratch2/gspacone/sensors_2026_paper_results_neworder/models/inter_session_ft/S01/silent/speechnet_padded/model_1"
model_name_id : "model_1"

# If true, use model_parent_directory exactly as-is (single-run mode).
# If false, sweep mode: derive the base path from the sweep subject/condition.
use_yaml_model_parent_directory: false

# Name of the specific model to fine-tune.
# If null, fine-tune all expected base models in the directory.
model_ft_name: ""

# Batch fine-tuning scheme
# "base": progressive batches (zero-shot batch1 -> FT batch1 -> zero-shot batch2 -> FT batch2 ...)
# "single_batch": fine-tune on a single batch (requires single_batch_id)
batch_ft_scheme: "base"

# Batch ID used only if batch_ft_scheme == "single_batch"
single_batch_id: none

# Whether to re-train the inter-session models from scratch,
# even if a parent model is provided.
retrain_intersessions: false

# true => base model in all_subjects folder, FT still per-subject
pretrain_all_subjects: false   

